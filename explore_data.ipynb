{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data with DataFrames and Spark SQL\n",
    "In this case, I will explore data using the Spark DataFrames API and Spark SQL.\n",
    "\n",
    "## Load Data Using an Explicit Schema\n",
    "To explore data, I must load it into a programmatic data object such as a DataFrame. If the structure of the data is known ahead of time, I can explicitly specify the schema for the DataFrame.\n",
    "In this exercise, I will work with data that records details of flights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightSchema = StructType([\n",
    "  StructField(\"DayofMonth\", IntegerType(), False),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
    "  StructField(\"Carrier\", StringType(), False),\n",
    "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
    "  StructField(\"DestAirportID\", IntegerType(), False),\n",
    "  StructField(\"DepDelay\", IntegerType(), False),\n",
    "  StructField(\"ArrDelay\", IntegerType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flights = spark.read.csv('/Users/EDHLINA/Downloads/DAT202/data/raw-flight-data.csv', schema=flightSchema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|\n",
      "|        19|        5|     DL|          10397|        15016|      -1|     -19|\n",
      "|        19|        5|     DL|          15016|        10397|       0|      -1|\n",
      "|        19|        5|     DL|          10397|        14869|      15|      24|\n",
      "|        19|        5|     DL|          10397|        10423|      33|      34|\n",
      "|        19|        5|     DL|          11278|        10397|     323|     322|\n",
      "|        19|        5|     DL|          14107|        13487|      -7|     -13|\n",
      "|        19|        5|     DL|          11433|        11298|      22|      41|\n",
      "|        19|        5|     DL|          11298|        11433|      40|      20|\n",
      "|        19|        5|     DL|          11433|        12892|      -2|      -7|\n",
      "|        19|        5|     DL|          10397|        12451|      71|      75|\n",
      "|        19|        5|     DL|          12451|        10397|      75|      57|\n",
      "|        19|        5|     DL|          12953|        10397|      -1|      10|\n",
      "|        19|        5|     DL|          11433|        12953|      -3|     -10|\n",
      "|        19|        5|     DL|          10397|        14771|      31|      38|\n",
      "|        19|        5|     DL|          13204|        10397|       8|      25|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer a Data Schema\n",
    "If the structure of the data source is unknown, you can have Spark auomatically infer the schema.\n",
    "In this case, I will load data about airports without knowing the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "|     10304|      Aniak|   AK|       Aniak Airport|\n",
      "|     10754|     Barrow|   AK|Wiley Post/Will R...|\n",
      "|     10551|     Bethel|   AK|      Bethel Airport|\n",
      "|     10926|    Cordova|   AK|Merle K Mudhole S...|\n",
      "|     14709|  Deadhorse|   AK|   Deadhorse Airport|\n",
      "|     11336| Dillingham|   AK|  Dillingham Airport|\n",
      "|     11630|  Fairbanks|   AK|Fairbanks Interna...|\n",
      "|     11997|   Gustavus|   AK|    Gustavus Airport|\n",
      "|     12523|     Juneau|   AK|Juneau International|\n",
      "|     12819|  Ketchikan|   AK|Ketchikan Interna...|\n",
      "|     10245|King Salmon|   AK| King Salmon Airport|\n",
      "|     10170|     Kodiak|   AK|      Kodiak Airport|\n",
      "|     13970|   Kotzebue|   AK| Ralph Wien Memorial|\n",
      "|     13873|       Nome|   AK|        Nome Airport|\n",
      "|     14256| Petersburg|   AK|Petersburg James ...|\n",
      "|     14828|      Sitka|   AK|Sitka Rocky Gutie...|\n",
      "|     12807| St. Mary's|   AK|  St. Mary's Airport|\n",
      "|     11445|   Unalaska|   AK|    Unalaska Airport|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports = spark.read.csv('/Users/EDHLINA/Downloads/DAT202/data/airports.csv', inferSchema=True, header=True)\n",
    "airports.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use DataFrame Methods\n",
    "Spark DataFrames provide functions that you can use to extract and manipulate data. For example, you can use the select function to return a new DataFrame containing columns selected from an existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|       city|                name|\n",
      "+-----------+--------------------+\n",
      "|Adak Island|                Adak|\n",
      "|  Anchorage|Ted Stevens Ancho...|\n",
      "|      Aniak|       Aniak Airport|\n",
      "|     Barrow|Wiley Post/Will R...|\n",
      "|     Bethel|      Bethel Airport|\n",
      "|    Cordova|Merle K Mudhole S...|\n",
      "|  Deadhorse|   Deadhorse Airport|\n",
      "| Dillingham|  Dillingham Airport|\n",
      "|  Fairbanks|Fairbanks Interna...|\n",
      "|   Gustavus|    Gustavus Airport|\n",
      "|     Juneau|Juneau International|\n",
      "|  Ketchikan|Ketchikan Interna...|\n",
      "|King Salmon| King Salmon Airport|\n",
      "|     Kodiak|      Kodiak Airport|\n",
      "|   Kotzebue| Ralph Wien Memorial|\n",
      "|       Nome|        Nome Airport|\n",
      "| Petersburg|Petersburg James ...|\n",
      "|      Sitka|Sitka Rocky Gutie...|\n",
      "| St. Mary's|  St. Mary's Airport|\n",
      "|   Unalaska|    Unalaska Airport|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities = airports.select(\"city\", \"name\")\n",
    "cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Operations\n",
    "You can combine functions in a single statement to perform multiple operations on a DataFrame. In this case, I will use the **join** function to combine the **flights** and **airports** DataFrames, and then use the **groupBy** and **count** functions to return the number of flights from each airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|             city| count|\n",
      "+-----------------+------+\n",
      "|          Phoenix| 90281|\n",
      "|            Omaha| 13537|\n",
      "|   Raleigh/Durham| 28436|\n",
      "|        Anchorage|  7777|\n",
      "|           Dallas| 19503|\n",
      "|          Oakland| 25503|\n",
      "|      San Antonio| 23090|\n",
      "|     Philadelphia| 47659|\n",
      "|       Louisville| 10953|\n",
      "|Dallas/Fort Worth|105024|\n",
      "|      Los Angeles|118684|\n",
      "|       Sacramento| 25193|\n",
      "|     Indianapolis| 18099|\n",
      "|        Cleveland| 25261|\n",
      "|        San Diego| 45783|\n",
      "|    San Francisco| 84675|\n",
      "|        Nashville| 34927|\n",
      "|    Oklahoma City| 13967|\n",
      "|          Detroit| 62879|\n",
      "|         Portland| 30640|\n",
      "+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsByOrigin = flights.join(airports, flights.OriginAirportID == airports.airport_id).groupBy(\"city\").count()\n",
    "flightsByOrigin.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Rows in a DataFrame\n",
    "Now that you're familiar with working with DataFrames, a key task when building predictive solutions is to explore the data, determing statistics that will help you understand the data before building predictive models. For example, how many rows of flight data do you actually have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2719418"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Summary Statistics\n",
    "Predictive modeling is based on statistics and probability, so you will often start by looking at summary statistics. The **describe** function returns a DataFrame containing the **count**, **mean**, **standard deviation**, **minimum**, and **maximum** values for each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|       DayofMonth|         DayOfWeek|Carrier|   OriginAirportID|     DestAirportID|         DepDelay|         ArrDelay|\n",
      "+-------+-----------------+------------------+-------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|          2719418|           2719418|2719418|           2719418|           2719418|          2691974|          2690385|\n",
      "|   mean|15.79747468024408|3.8983907586108497|   null| 12742.26441172339|12742.455345592329|10.53686662649788| 6.63768791455498|\n",
      "| stddev|8.799860168985367|1.9859881390373617|   null|1501.9729397025808|1501.9692528927785|36.09952806643081|38.64881489390021|\n",
      "|    min|                1|                 1|     9E|             10140|             10140|              -63|              -94|\n",
      "|    max|               31|                 7|     YV|             15376|             15376|             1863|             1845|\n",
      "+-------+-----------------+------------------+-------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Presence of Duplicates\n",
    "The data I have to work with won't always be perfect - often I'll want to *clean* the data; for example to detect and remove duplicates that might affect my model. I can use the **dropDuplicates** function to create a new DataFrame with the duplicates removed, enabling me to determine how many rows are duplicates of other rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22435"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.count() - flights.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Missing Values\n",
    "As well as determing if duplicates exist in my data, I should detect missing values, and either remove rows containing missing data or replace the missing values with a suitable relacement. The **dropna** function creates a DataFrame with any rows containing missing data removed - I can specify a subset of columns, and whether the row should be removed in *any* or *all* values are missing. I can then use this new DataFrame to determine how many rows contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46233"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.count() - flights.dropDuplicates().dropna(how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "Now that I've identified that there are duplicates and missing values, I can clean the data by removing the duplicates and replacing the missing values. The **fillna** function replaces missing values with a specified replacement value. In this case, I'll remove all duplicate rows and replace missing **ArrDelay** and **DepDelay** values with **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2696983"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=flights.dropDuplicates().fillna(value=0, subset=[\"ArrDelay\", \"DepDelay\"])\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Summary Statistics\n",
    "After cleaning the data, you should re-check the statistics - removing rows and changing values may affect the distribution of the data, which in turn could affect any predictive models you might create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+------------------+-----------------+------------------+------------------+\n",
      "|summary|        DayofMonth|         DayOfWeek|Carrier|   OriginAirportID|    DestAirportID|          DepDelay|          ArrDelay|\n",
      "+-------+------------------+------------------+-------+------------------+-----------------+------------------+------------------+\n",
      "|  count|           2696983|           2696983|2696983|           2696983|          2696983|           2696983|           2696983|\n",
      "|   mean|15.798996508320593| 3.900369412784582|   null|12742.459424846207|12742.85937657004|10.531134234068217|6.6679285705545785|\n",
      "| stddev|  8.80126719913545|1.9864582421701973|   null|1502.0359941370602|1501.993958981798| 36.06172819056574| 38.58386147358071|\n",
      "|    min|                 1|                 1|     9E|             10140|            10140|               -63|               -94|\n",
      "|    max|                31|                 7|     YV|             15376|            15376|              1863|              1845|\n",
      "+-------+------------------+------------------+-------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Relationships in the Data\n",
    "Predictive modeling is largely based on statistical relationships between fields in the data. To design a good model, you need to understand how the data points relate to one another and identify any apparent correlation. The **corr** function calculates a correlation value between -1 and 1, indicating the strength of correlation between two fields. A strong positive correlation (near 1) indicates that high values for one column are often found with high values for the other, whith a string negative correlation (near -1) indicates that *low* values for one column are often found with *high* values for the other. A correlation near 0 indicates little apparent relationship between the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9392630367706958"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr(\"DepDelay\", \"ArrDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Spark SQL\n",
    "In addition to using the DataFrame API directly to query data, I can persist DataFrames as table and use Spark SQL to query them using the SQL language. SQL is often more intuitive to use when querying tabular data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|DayOfWeek|          AvgDelay|\n",
      "+---------+------------------+\n",
      "|        1| 7.077989660973244|\n",
      "|        2|  4.39237404158651|\n",
      "|        3| 7.234625279280266|\n",
      "|        4|10.775574715480056|\n",
      "|        5|  8.71110560964396|\n",
      "|        6|2.1437428120738304|\n",
      "|        7|  5.25403935972552|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView(\"flightData\")\n",
    "\n",
    "spark.sql(\"SELECT DayOfWeek, AVG(ArrDelay) AS AvgDelay FROM flightData GROUP BY DayOfWeek ORDER BY DayOfWeek\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|DepDelay|ArrDelay|\n",
      "+--------+--------+\n",
      "|      -5|     -16|\n",
      "|      -8|      -4|\n",
      "|      -4|       2|\n",
      "|      -1|     -23|\n",
      "|      -1|     -11|\n",
      "|     109|     106|\n",
      "|       0|     -11|\n",
      "|       3|      -1|\n",
      "|       1|     -12|\n",
      "|       2|     -12|\n",
      "|      -3|     -10|\n",
      "|      15|      11|\n",
      "|       0|     -19|\n",
      "|       6|      -8|\n",
      "|      -2|     -12|\n",
      "|       1|      61|\n",
      "|     140|     130|\n",
      "|      27|      13|\n",
      "|       5|       4|\n",
      "|      -9|     -18|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DepDelay, ArrDelay FROM flightData\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airports.createOrReplaceTempView(\"airportData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                name|           avgdelay|\n",
      "+--------------------+-------------------+\n",
      "|     Eppley Airfield|  9.023493663381013|\n",
      "|     Kahului Airport| 4.4546988807958785|\n",
      "|San Diego Interna...|  5.773734422383071|\n",
      "|            Bob Hope|    4.4999646568177|\n",
      "|Hartsfield-Jackso...|  7.278792574874507|\n",
      "|Sacramento Intern...| 5.8097549235891055|\n",
      "|Chicago O'Hare In...|  9.796627713118575|\n",
      "|   Will Rogers World| 10.492794350771005|\n",
      "|Ted Stevens Ancho...|-1.3845654993514915|\n",
      "|Raleigh-Durham In...|  6.549666761202496|\n",
      "|Minneapolis-St Pa...|  2.910011511319464|\n",
      "|Metropolitan Oakl...|   5.14337972166998|\n",
      "|Norman Y. Mineta ...|  5.055079508939606|\n",
      "|Southwest Florida...| 4.0729083025533965|\n",
      "|San Antonio Inter...|  8.470013521175906|\n",
      "|Cincinnati/Northe...|  4.714566846711049|\n",
      "|           LaGuardia| 11.209838721036693|\n",
      "|     William P Hobby|   7.54095066518847|\n",
      "|Philadelphia Inte...|  8.517859028789895|\n",
      "| Miami International| 3.5782458645721764|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT a.name, AVG(f.ArrDelay) AS avgdelay FROM flightData AS f JOIN airportData AS a ON f.DestAirportID = a.airport_id GROUP BY a.name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.registerTempTable(\"flightData2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|         1|        1|     9E|          13487|        11042|      -5|     -16|\n",
      "|         1|        1|     9E|          14122|        11193|      -8|      -4|\n",
      "|         1|        1|     9E|          14524|        11433|      -4|       2|\n",
      "|         1|        1|     AA|          10821|        11298|      -1|     -23|\n",
      "|         1|        1|     AA|          11042|        11298|      -1|     -11|\n",
      "|         1|        1|     AA|          11298|        10423|     109|     106|\n",
      "|         1|        1|     AA|          11298|        13495|       0|     -11|\n",
      "|         1|        1|     AA|          11298|        14057|       3|      -1|\n",
      "|         1|        1|     AA|          11298|        14679|       1|     -12|\n",
      "|         1|        1|     AA|          11298|        15304|       2|     -12|\n",
      "|         1|        1|     AA|          12266|        13303|      -3|     -10|\n",
      "|         1|        1|     AA|          12451|        11298|      15|      11|\n",
      "|         1|        1|     AA|          12892|        12264|       0|     -19|\n",
      "|         1|        1|     AA|          13198|        11298|       6|      -8|\n",
      "|         1|        1|     AA|          13204|        12892|      -2|     -12|\n",
      "|         1|        1|     AA|          13303|        12478|       1|      61|\n",
      "|         1|        1|     AA|          13303|        14771|     140|     130|\n",
      "|         1|        1|     AA|          14679|        13930|      27|      13|\n",
      "|         1|        1|     AA|          14730|        11298|       5|       4|\n",
      "|         1|        1|     AS|          13930|        14747|      -9|     -18|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM flightData2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
